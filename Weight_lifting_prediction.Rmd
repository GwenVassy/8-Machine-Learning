---
title: "Practical Machine Learning - Weight Lifting Exercises Prediction"
author: "Gwen Vassy"
date: "01/03/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
<br><br>

## 1. Introduction

One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, we will use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to determine how well they are performing a barbell lift.

The goal of our project is to predict the manner in which they did the exercise (a level from A to D). This is the "classe" variable in the training set. We will first decide which variables to predict with. We will then describe how we built our model, how we used cross validation, what we think the expected out of sample error is, and why we made the choices we did. 

The data used in this assignment can be found here:  

* **Training data**: <https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv>  
* **Testing data**:  <https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv>  

We first download the data from the sources and store them in a training and testing set

```{r library, echo=FALSE, include=FALSE}
library(knitr)
library(caret)
library(rpart)
library(scales)
library(beepr)
library(corrplot)
library(randomForest)
library(MASS)
library(caretEnsemble)
library(rpart.plot)
```

```{r data}
# read in two files: training and testing
data <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")
```
<br><br>

We will then perform the following steps:  

* Explore the data, graphically and quantitatively
* Clean the data
* Partition the data into different sets  
        + Training set 1 to build the models (algorithms)  
        + Training set 2 to compare the models and select the most appropriate  
        + Validation set to perform cross validation (i.e. test the accuracy of the selected model)  
* Fit the different models
* Cross validate the chosen model


## 2. Exploratory Analysis

We briefly explore the data...  
```{r explore}
str(data, list.len = 10)
```

We then want to check quickly how 'clean' the raw data is. We calculate the proportion of NAs in the raw data, which is:

```{r nas }
# check proportion of data elements that are "NA"
percent( sum(is.na(data)) / (nrow(data)*ncol(data)) )
```

Given this data has a lot of NAs, we will clean it up first and then partition it into a training set and a validation set (for model selection).
<br><br>

## 3. Cleaning the data

We check how many of the rows are 'complete'
```{r complete }
# check number of complete cases
sum(complete.cases(data))
```

Given most rows are not complete, we cannot simply remove all complete cases, as this would only leave us with 406 observations, which would then need to be further split into training and validation. We therefore need to take a more elaborate approach to cleaning our data.
<br><br>

#### 3.1 Remove mostly NA variables

First we check if there are any variables which do not contain a lot of values
```{r nacols }
# calculate proportion of data items that are "NA"
proportionNA <- percent(sapply(data, function(x) mean(is.na(x))))
table(proportionNA)
```

We can see that while a large number of columns have no NAs at all (93 have proportion 0), there are a significant number of columns that are almost all NAs (67 have over 97% NAs). These colums are unlikely to be good predictors so our first step is to remove them.
<br>
```{r remove }
# check dimensions of the data
dim(data)
# remove columns where the proportion of NAs is higher than 90%
data <- data[, proportionNA < 0.9]
# check dimensionso of the new data
dim(data)
```

Our new data now only has **93 variables.** We can check again the proportion of NAs and complete cases...  
<br>

```{r completecases}
# number of complete cases
sum(complete.cases(data))
# proportion of data elements that are "NA"
percent( sum(is.na(data)) / (nrow(data)*ncol(data)) )
```

**The data is now free of NAs and all rows are complete.**
<br><br>

#### 3.2 Remove mostly zero variables or near zero variance

Some variables (columns) still have a high proportion of zeros / very low variance, which we need to remove.
We also remove the first 6 columns which contain mostly identifiers.

```{r small}
# remove first 6 columns, and all columns that have near zero variance
data <- data[ , -c(1:6,nearZeroVar(data))]
# check dimensions of the new data
dim(data)
```

We have 54 useful columns left, all of which potentially useful predictors. 

However we may have strong collinearity between some variables, which should be removed before fitting some types of models (e.g. lda)
<br><br>

#### 3.3 Check collinear variables

We will now explore how correlated the different variables are to one another.
```{r collin}
corr.matrix <- cor(data[, -54])
# Return the number of columns which have a correlation >0.9 or <-0.9 to another column
length(findCorrelation(corr.matrix, cutoff = 0.9))
```
We can plot the correlation matrix for a more graphical analysis.  

```{r matrix}
corrplot(corr.matrix, tl.cex = 0.5, tl.col = rgb(0, 0, 0))
```

While there are 7 variables which are more than 90% correlated to another variable and could be removed, there are only a few of these so a PCA analysis is not warranted as it reduces interpretability.

Removing these columns is not warranted either, as these do contain additional data, and are not in large enough numbers to severly affect computing times.

<br><br>

## 4. Partition the data into training sets and a validation set

```{r part}
set.seed(1709)
inTrain <- createDataPartition(data$classe, p = 0.8)[[1]]
training = data[inTrain, ]
validation = data[-inTrain, ]
```

We then further partition the training set into a training 1 set (for initial fitting), and a training 2 set, for model selection

```{r part2}
set.seed(1709)
inTrain <- createDataPartition(training$classe, p = 0.5)[[1]]
training.1 = training[inTrain, ]
training.2 = training[-inTrain, ]
```

```{r rm, echo = FALSE}
rm(inTrain, proportionNA, corr.matrix)
```
<br><br>

## 5. Fitting a prediction model

We will now proceed to training a model to the training set. We will explore different models and assess these against the validation set before picking the best one, which will then be tested (only once) against the validation set.  

<br><br>

### 5.1 Brute force approach (using all variables)

We can simply run different models on the complete data and select the most accurate one. However this is not the most efficient way to fit a model (or indeed necessarily the most accurate), so we will proceed to variable selection in the next section.

<br><br>

#### 5.1.1 Decicion tree

First we set up our train controls for validation to be of the repeated k-fold cross validation type, in this case 3 repetitions of 10-fold validation.
```{r control}
control <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
```

We then train the model, and fit the model to the validation set to determine its accuracy

     
```{r tree}
# 1. Fit a decision tree using the "rpart" method        
        set.seed(1709)
        mod.tree <- train(classe ~., method = "rpart", data = training.1, trControl=control)
        pred.tree <- predict(mod.tree, training.2)
        cat("Accuracy of Decision Tree:", postResample(pred.tree, training.2$classe)[1], "\n")
```
We can plot this classification tree, although it is really not very accurate
```{r treeplot, echo = FALSE, include = FALSE}
rattle::fancyRpartPlot(mod.tree$finalModel)
```  

<br><br>

#### 5.1.2 Linear Discriminant Analysis
```{r lda}        
# 2. Fit a linear discriminant analysis using the "lda" method
        set.seed(1709)
        mod.lda <- train(classe ~., method = "lda", data = training.1, 
                         verbose = FALSE, trControl=control, warnings=FALSE)
        pred.lda <- predict(mod.lda, training.1)
        cat("Accuracy of lda:", postResample(pred.lda, training.2$classe)[1], "\n")
```

<br><br>

#### 5.1.3 Random forest
```{r rf}              
# 3. Fit a random forest predictor using the "rf method"
        set.seed(1709)
        mod.rf <- randomForest(classe ~., data = training.1, trControl=control)
        pred.rf <- predict(mod.rf, training.2)
        cat("Accuracy of Random Forest:", postResample(pred.rf, training.2$classe)[1], "\n")
        
```
<br> We can also plot the convergence of this Random Forest model:

```{r}
plot(mod.rf)
```

We note that the Random Forest model is by far the most accurate, although a combination of all three models may be even more accurate. The Random Forest model also tells us which variables are most important. It also tells us that the model does not get significantly more accurate after approximately 150 to 200 trees.

    
```{r plotimport}  
varImpPlot(mod.rf, cex = 0.8)

```

<br><br>

### 5.2 Selected variables

As a general rule, if accuracy is not compromised, it is better to have a simpler model (which is more intuitive) than a model with many more variables. We can therefore select the most important variables and train a model to these.  

According to the Gini index of the Random Forest we ran, the 5 most important variables are:  

```{r import}  
# Return the 5 most important variables according to the Gini index
import.var <- importance(mod.rf)
head(import.var, 10)
```

We can now fit a new Random Forest algorithm to these variables, in order to check whether a simpler model may be just as adequate as the brute force approach.  

First we want to check the collinearity of our remaining variables  
```{r collin2}      
corr.matrix2 <- cor(training.1[, c(rownames(import.var)[1:5])])
# Plot the correlation matrix
corrplot(corr.matrix2, tl.cex = 0.8, tl.col = rgb(0, 0, 0))
```

We notice that total_accel_belt and roll_belt seem highly correlated so we check this pair's correlation
```{r cor}  
cor(training.1$roll_belt, training.1$total_accel_belt)
```
Given these two variables are very highly correlated (
`r percent(cor(training.1$roll_belt, training.1$total_accel_belt))`) we should remove one of them. roll_belt scores higher on the Gini index so we remove total_accel_belt from our model. Our remaining variables are:  

```{r}
rownames(import.var)[c(1:4)]
```



```{r rf2}              
# Fit a random forest predictor using the "rf"" method to the selected variables
        set.seed(1709)
        
        imp.var <- paste0(rownames(import.var)[c(1:4)], collapse = "+")
        formula.rf <- as.formula(paste0("classe ~", imp.var))
        mod.rf.simple <- randomForest(formula.rf, data = training.1, ntree = 200, trControl=control)
        pred.rf.simple <- predict(mod.rf.simple, training.2)
        cat("Accuracy of simplified Random Forest:", postResample(pred.rf.simple, training.2$classe)[1], "\n")
```

We can also plot this Random Forest convergence

```{r}
plot(mod.rf.simple, lty=1, lwd=c(3,1,1,1,1,1), 
     main = "Convergence of Simplified Random Forest Model"); legend(160, y=0.042, 
       legend = c("OOB", "A","B", "C", "D", "E"), 
       col = c("black", "red", "green", "blue", "cyan", "magenta"),
       lty=1, lwd=c(3,1,1,1,1,1),
       cex=0.8,
       bty = "n",
       seg.len = 3)
```
This seems to be a very accurate model (in fact at
`r percent(postResample(pred.rf.simple, training.2$classe)[1])`, it is more accurate than the model based on all the variables at 
`r percent(postResample(pred.rf, training.2$classe)[1])`
), based only on these 4 variables, and is thus a better model: it is both *simpler* and *more accurate*).

We therefore select this model as our **final** model.

## 6. Cross Validation

### 6.1 Accuracy of the model

As previously mentioned, this model is very accurate on the training2 set, which is why it was selected. Now we use cross validation, i.e. we test the model on a data set different from the one on which the model was built and select: in our case the VALIDATION data set. 

```{r val}
pred.rf.val <- predict(mod.rf.simple, validation)
confusionMatrix(pred.rf.val, validation$classe)
```
  
We can see that the accuracy is still `r percent(postResample(pred.rf.simple, training.2$classe)[1])`, which is excellent.

<br><br>

### 6.2 Cross Validation: Expected out of sample error

The expected out of sample error on the validation data set is the number of mismatches on the validation data, which can be calculated like this:

```{r}
sum(pred.rf.val != validation$classe) / length(validation$classe)
```
Our predicted out-of-sample error is **
`r percent(sum(pred.rf.val != validation$classe) / length(validation$classe))`
**

**We can now apply this model to the test set of the assignment**
<br><br>



**Reference**:
Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

Read more: http://groupware.les.inf.puc-rio.br/har#wle_paper_section#ixzz4aBNnXKBk


