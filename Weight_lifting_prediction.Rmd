---
title: "Practical Machine Learning"
title2: "Weight Lifting Exercises Prediction"
author: "Gwen Vassy"
date: "01/03/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
<br><br>

## 1. Introduction

One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, we will use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants.

The goal of our project is to predict the manner in which they did the exercise (a level from A to D). This is the "classe" variable in the training set. We will first decide which variables to predict with. We will then describe how we built our model, how we used cross validation, what we think the expected out of sample error is, and why we made the choices we did. 

The data used in this assignment can be found here:  

* **Training data**: <https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv>  
* **Testing data**:  <https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv>  

We first download the data from the sources and store them in a training and testing set

```{r library, echo=FALSE, include=FALSE}
library(knitr)
library(caret)
library(rpart)
library(scales)
library(beepr)
library(corrplot)
library(randomForest)
library(MASS)
library(caretEnsemble)
library(rpart.plot)
```

```{r data}
data <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
testing <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv")
```
<br><br>

## 2. Exploratory Analysis

We briefly explore the data...  
```{r explore}
str(data, list.len = 10)
```

We then want to check quickly how 'clean' the raw data is. We calculate the proportion of NAs in the raw data, which is:

```{r nas }
percent( sum(is.na(data)) / (nrow(data)*ncol(data)) )
```

Given this data has a lot of NAs, we will clean it up first and then partition it into a training set and a validation set (for model selection).
<br><br>

## 3. Cleaning the data

We check how many of the rows are 'complete'
```{r complete }
sum(complete.cases(data))
```

Given most rows are not complete, we cannot simply remove all complete cases, as this would only leave us with 406 observations, which would then need to be further split into training and validation. We therefore need to take a more elaborate approach to cleaning our data.
<br><br>

#### 3.1 Remove mostly NA variables

First we check if there are any variables which do not contain a lot of values
```{r nacols }
proportionNA <- percent(sapply(data, function(x) mean(is.na(x))))
table(proportionNA)
```

We can see that while a large number of columns have no NAs at all (93 have proportion 0), there are a significant number of columns that are almost all NAs (67 have over 97% NAs). These colums are unlikely to be good predictors so our first step is to remove them.
```{r remove }
dim(data)
data <- data[, proportionNA < 0.9]
dim(data)
```
Our new data now only has 93 variables. We can check again the proportion of NAs and complete cases...  

```{r completecases}
sum(complete.cases(data))
percent( sum(is.na(data)) / (nrow(data)*ncol(data)) )
```

The data is now free of NAs and all rows are complete.
<br><br>

#### 3.2 Remove mostly zero variables or near zero variance

Some variables (columns) still have a high proportion of zeros / very low variance, which we need to remove.
We also remove the first 6 columns which contain mostly identifiers.

```{r small}
data <- data[ , -c(1:6,nearZeroVar(data))]
dim(data)
```

We have 54 useful columns left, all of which potentially useful predictors. 

However we may have strong collinearity between some variables, which should be removed before fitting some types of models (e.g. lda)
<br><br>

#### 3.3 Check collinear variables

We will now explore how correlated the different variables are to one another.
```{r collin}
corr.matrix <- cor(data[, -54])
# Return the number of columns which have a correlation >0.9 or <-0.9 to another column
length(findCorrelation(corr.matrix, cutoff = 0.9))
```
We can plot the correlation matrix for a more graphical analysis.  

```{r matrix}
corrplot(corr.matrix, tl.cex = 0.5, tl.col = rgb(0, 0, 0))
```

While there are 7 variables which are more than 90% correlated to another variable and could be removed, there are only a few of these so a PCA analysis is not warranted as it reduces interpretability.

Removing these columns is not warranted either, as these do contain additional data, and are not in large enough numbers to severly affect computing times.

<br><br>

## 4. Partition the data into training sets and a validation set

```{r part}
set.seed(1709)
inTrain <- createDataPartition(data$classe, p = 0.8)[[1]]
training = data[inTrain, ]
validation = data[-inTrain, ]
```

We then further partition the training set into a training 1 set (for initial fitting), and a training 2 set, for model selection

```{r part2}
set.seed(1709)
inTrain <- createDataPartition(training$classe, p = 0.5)[[1]]
training.1 = training[inTrain, ]
training.2 = training[-inTrain, ]
```

```{r rm, echo = FALSE}
rm(inTrain, proportionNA, corr.matrix)
```
<br><br>

## 5. Fitting a prediction model

We will now proceed to training a model to the training set. We will explore different models and assess these against the validation set before picking the best one, which will then be tested (only once) against the validation set.  

<br><br>

### 5.1 Brute force approach (using all variables)

We can simply run different models on the complete data and select the most accurate one. However this is not the most efficient way to fit a model (or indeed necessarily the most accurate), so we will proceed to variable selection in the next section.

<br><br>

#### 5.1.1 Decicion tree

First we set up our train controls for validation to be of the repeated k-fold cross validation type, in this case 3 repetitions of 10-fold validation.
```{r control}
control <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
```

We then train the model, and fit the model to the validation set to determine its accuracy

     
```{r tree}
# 1. Fit a decision tree using the "rpart" method        
        set.seed(1709)
        mod.tree <- train(classe ~., method = "rpart", data = training.1, trControl=control)
        pred.tree <- predict(mod.tree, training.2)
        cat("Accuracy of Decision Tree:", postResample(pred.tree, training.2$classe)[1], "\n")
```
We can plot this classification tree, although it is really not very accurate
```{r treeplot, echo = FALSE, include = FALSE}
rattle::fancyRpartPlot(mod.tree$finalModel)
```  

<br><br>

#### 5.1.2 Linear Discriminant Analysis
```{r lda}        
# 2. Fit a linear discriminant analysis using the "lda" method
        set.seed(1709)
        mod.lda <- train(classe ~., method = "lda", data = training.1, 
                         verbose = FALSE, trControl=control, warnings=FALSE)
        pred.lda <- predict(mod.lda, training.1)
        cat("Accuracy of lda:", postResample(pred.lda, training.2$classe)[1], "\n")
```

<br><br>

#### 5.1.3 Random forest
```{r rf}              
# 3. Fit a random forest predictor using the "rf method"
        set.seed(1709)
        mod.rf <- randomForest(classe ~., data = training.1, trControl=control)
        pred.rf <- predict(mod.rf, training.2)
        cat("Accuracy of Random Forest:", postResample(pred.rf, training.2$classe)[1], "\n")
        
```

We note that the Random Forest model is by far the most accurate, although a combination of all three models may be even more accurate. The Random Forest model also tells us which variables are most important.

    
```{r plotimport}  
varImpPlot(mod.rf, cex = 0.8)

```

<br><br>

### 5.2 Selected variables

As a general rule, if accuracy is not compromised, it is better to have a simpler model (which is more intuitive) than a model with many more variables. We can therefore select the most important variables and train a model to these.  

According to the Gini index of the Random Forest we ran, the 5 most important variables are:  

```{r import}  
# Return the 5 most important variables according to the Gini index
import.var <- importance(mod.rf)
head(import.var, 10)
```

We can now fit a new Random Forest algorithm to these variables, in order to check whether a simpler model may be just as adequate as the brute force approach.  

First we want to check the collinearity of our remaining variables  
```{r collin2}      
corr.matrix2 <- cor(training.1[, c(rownames(import.var)[1:5])])
# Plot the correlation matrix
corrplot(corr.matrix2, tl.cex = 0.8, tl.col = rgb(0, 0, 0))
```

We notice that total_accel_belt and roll_belt seem highly correlated so we check this pair's correlation
```{r cor}  
cor(training.1$roll_belt, training.1$total_accel_belt)
```
Given these two variables are very highly correlated ( `r percent(cor(training.1$roll_belt, training.1$total_accel_belt))` ) we should remove one of them. roll_belt scores higher on the Gini index so we remove total_accel_belt from our model. Our remaining variables are:  

```{r}
rownames(import.var)[c(1:4)]
```



```{r rf2}              
# Fit a random forest predictor using the "rf"" method to the selected variables
        imp.var <- paste0(rownames(import.var)[c(1:4)], collapse = "+")
        formula.rf <- as.formula(paste0("classe ~", imp.var))
        mod.rf.simple <- randomForest(formula.rf, data = training.1, trControl=control)
        pred.rf.simple <- predict(mod.rf.simple, training.2)
        cat("Accuracy of simplified Random Forest:", postResample(pred.rf.simple, training.2$classe)[1], "\n")
```

This seems to be a very accurate model (in fact at 99.6%, it is more accurate than the model based on all the variables at 99.2%), based only on these 4 variables, and is thus a better model: it is both *simpler* and *more accurate*).

We therefore select this model as our **final** model.

## 6. Model Validation

### 6.1 Accuracy of the model

As previously mentioned, this model is very accurate on the training2 set, which is why it was selected. Now that we have selected it, lets examine its accuracy on the validation set:

```{r val}
        pred.rf.val <- predict(mod.rf.simple, validation)
        confusionMatrix(pred.rf.val, validation$classe)
```
  
We can see that the accuracy is still 99.6%, which is exxcellent.

<br><br>

### 6.2 Expected out of sample error

The expected out of sample error on the validation data set is the number of mismatches on the validation data, which can be calculated like this:

```{r}
sum(pred.rf.val != validation$classe) / length(validation$classe)
```
Our predicted out-of-sample error is **0.4%**.

**We can now apply this model to the test set of the assignment**
<br><br>



*IMPORTANT NOTE: In reality, the num_window variable seems too highly correlated to the classe variable to be a real predictor and may be a factor of how the experiment was recorded.* <br>

*It should probably not be included in the analysis for the purposes of real predictions. However for the purpose of the exercise, which specifies "You may use any of the other variables to predict with", it has been kept in the model as it is by far the best predictor.*

**Reference**:
Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

Read more: http://groupware.les.inf.puc-rio.br/har#wle_paper_section#ixzz4aBNnXKBk


